\section{Implementation}

This section describes details of Sprocket's design and implementation. These matters are only
significant to those who are interested in understanding how Sprocket works or who want to
further develop Sprocket. If you are only interested in using Sprocket, you can skip this
section.

\subsection{Certificate Processing}
\label{sec:rt0-certificate-processing}

Each node in the network contains a certificate sender component and a certificate receiver
component. These components transfer $RT_0$ certificates between the nodes and tend to the
details of verifying and storing them. Neither of these components provide any interfaces.
Instead they run as background daemons, performing their function asynchronously to the rest of
the system. Figure~\ref{fig:certificate-daemon} shows the certificate processing architecture of
a node.

\begin{figure}[htbp]
  \input{Figures/Certificate-Daemon}
  \centerline{\box\graph}
  \caption{Certificate Processing Architecture}
  \label{fig:certificate-daemon}
\end{figure}

At boot time the certificate sender starts a periodic timer. When the timer fires, the node
broadcasts all certificates in its certificate storage. The content of the storage is a
collection of read-only certificates generated by Sprocket. This storage represents the set of
certificates known to the node at deployment time and it is considered a fixed set.

The certificate sender never tries to broadcast certificates the node has received from others.
Doing so would cause every certificate to flood the network, creating excessive network traffic
and computational workload. Since SpartanRPC is a link level RPC mechanism there is little point
in a node's certificates propagating beyond that node's immediate neighbors.

Sprocket assumes that the contents of certificates are not secret so there is no security
problem with broadcasting them periodically to all listeners. In some environments a client
might wish to withhold information about the certificates it possesses until some sort of trust
negotiation protocol is executed with the server. However, in a sensor network context there is
little point in such negotiations because any node in radio range will be able to hear whatever
certificates are eventually broadcast. \pcnote{It is possible that the result of trust
  negotiation could be a shared key that could then be used to transmit selected certificates in
  an encrypted manner. Sprocket does not attempt anything like this.}

The largest certificate form (intersection role) is 166 bytes. Not all radios can transmit so
much data in a single packet and, even if such large packets are supported, it is wasteful of
memory to increase the packet size to that degree. Consequently the certificates are fragmented
into four fragments when they are sent. These fragments are sent back to back with just a small
delay to allow the receiver time to assemble them. \pcnote{Should I say something about the lack
  of fragment identifiers and how that is handled?}

To facilitate the handling of certificates that are damaged during transmission, the sender
appends a 16~bit Fletcher checksum to the end of each certificate. The receiver verifies this
checksum before attempting an expensive signature verification. In addition the receiver uses
the checksum as a key into its local credential storage. If the receiver has already processed
a certificate with the same checksum it assumes the certificate has already been seen and no
further processing of the incoming certificate is done.

Once a received certificate is known to be new and has been verified, the credential it contains
is extracted and stored. Public keys representing $RT_0$ entities are stored in a key storage
component; all references to those keys in the credential storage are by way of a small integer
index. This saves memory in the common case where the same key is used in multiple credentials.

The credential storage also contains the $RT_0$ minimum model implied by the currently known
collection of credentials. \pcnote{Currently the credentials in the certificate storage are not
  considered in the construction of the minimum model. I don't think this is a problem. The
  certificate storage holds credentials the node owns for accessing remote services, and the
  credential storage holds credentials other nodes are using to get access to local services. It
  is likely these credentials don't overlap. Yes?} Each time a new credential is added to
storage, the minimum model is updated. If the key storage overflows, credentials using the
removed keys are also removed. Furthermore the minimum model is rebuilt from scratch (to ensure
soundness) whenever a credential is removed, either because of a removed key or because of
credential storage overflow.

\subsection{Session Key Negotiation}

Public key cryptography is much too computationally expensive to use for routine duty postings.
Sprocket's run time system addresses this by negotiating session keys between the sender
(client) and receiver (server) nodes. Figure~\ref{fig:sessionkey-daemon} shows the session key
processing architecture of a node.

\begin{figure}[htbp]
  \input{Figures/SessionKey-Daemon}
  \centerline{\box\graph}
  \caption{Session Key Processing Architecture}
  \label{fig:sessionkey-daemon}
\end{figure}

The client maintains a session key storage that is indexed by the triple $(N, C, I)$ where $N$ is
the remote node ID, $C$ is the remote component ID, and $I$ is the remote interface ID. Session
keys are thus created for each combination of these IDs. It is possible for there to be several
different session keys with the same server node. This is necessary because a server node might
provide services with different access requirements\footnote{We expect this to be likely.}. The
server node will not want a client that can legitimately access one of its services to be able
to use the same session key to access some other, more restrictive service on the same node.

Notice that each remote server has its own session key so that outgoing messages that are
fanning out to multiple servers will need to have multiple MACs. This is a serious overhead
problem. \pcnote{Can this be improved by using some sort of multi-recipient signature scheme?}

The server will also need to maintain a session key cache indexed by $(N, C, I)$. In this case
$N$ is the node ID of the client and $C$, $I$ are the component and interface IDs on the server
to which those clients are communicating. Since most nodes will be playing the role of both
server and client, this implies that the each session key cache entry will also need a flag to
indicate if that entry is for a remote server or a remote client.

Alternatively two session key caches could be used, but that approach is not taken because it is
inefficient in its use of memory in the common case when a node is primarly a server or primarly
a client. Note that because duty postings in Spartan RPC are unidirectional, there is no apriori
relationship between outgoing and incoming requests. Each session key applies to only one
direction of communication.

The server manages the session keys in the SessionKeyCache component. When a message is received
from a client node $N$ containing the public key $K_{cp}$ of the sending node and the $(C, I)$
address of the desired service the following steps are taken

\begin{enumerate}
\item Authorization of $K_{cp}$ for service $(C, I)$ is checked using the $RT_0$ minimum model
  computed by the certificate receiver. If authorization fails nothing more is done.
\item A session key is computed using Eliptic Curve Diffie-Hellman and stored in the session key
  cache under the proper $(N, C, I)$ value. The key is stored as a remote client key.
\item A message is returned to the client containing the server's $K_{sp}$ and the original $(N,
  C, I)$ values used by the client. This is so the client is able to compute the same session
  key and associate it with the proper endpoint from its perspective.
\end{enumerate}

Note that every session key computed between nodes $N_A$ and $N_B$ will be the same. However
this is not as big a problem as it seems since the receiving node uses $(C, I)$ to look up the
session key. If a node attempts to access an unauthorized service, no entry for that $(C, I)$
will exist in the server's session key cache and access will fail.

It is also important to note that the server does not need to be authenticated; only the client.
This happy situation arises because duties do not return a value. Thus the client can't be
influenced by a rogue server. Clients do not care if a rogue server receives their requests
because nothing can be done about it anyway in the wireless medium. Clients could use encryption
to protect themselves against eavesdropping but that is outside the scope of the current system.

If the server does need to return a value it must do so by posting a duty on the client. In that
case the roles (client vs server) of the two nodes are reverse and client authentication remains
sufficient.

\subsection{Dynamic Wires}

Each remotable component\footnote{A ``remotable component'' is a component that provides at
least one remote interface.} is augmented with a Sprocket generated skeleton that receives
packets from the radio subsystem, decodes invocation information (command identity and
arguments) and calls the commands in the original component.

Each dynamic wire is\ldots

% I need to talk about the specifics of how the stubs and skeletons are generated here.
